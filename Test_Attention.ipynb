{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###change data set into [('old', 'new'), (...]\n",
    "json_filename = '/Users/yihe/Desktop/Stanford/yi_can_summarize/data/cnn/data.json'\n",
    "sizes_of_text, sizes_of_summary= [] , []\n",
    "with open(json_filename, 'r') as f:\n",
    "    data = json.load(f)\n",
    "all_data = []\n",
    "for key in data.keys():\n",
    "    if len(data[key])>1:\n",
    "        all_data.append((data[key][0].strip(), data[key][1].strip()))\n",
    "        \n",
    "        size_of_text = len(data[key][0].strip().split(' '))\n",
    "        size_of_summary = len(data[key][1].strip().split(' '))\n",
    "        sizes_of_text.append(size_of_text)\n",
    "        sizes_of_summary.append(size_of_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 14\n"
     ]
    }
   ],
   "source": [
    "Tx, Ty = int(np.percentile(sizes_of_text, 75)), int(np.percentile(sizes_of_summary, 75))\n",
    "print Tx, Ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "m = len(all_data)\n",
    "print m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change text into {'word':indx}\n",
    "def create_vocab_list(vocab_text):\n",
    "    vocab_list={}\n",
    "\n",
    "    vocab_filename = \"/Users/yihe/Desktop/Stanford/yi_can_summarize/data/cnn/vocab.txt\"\n",
    "    with open(vocab_text, 'r') as vocab_f:\n",
    "        i=0\n",
    "        for line in vocab_f:\n",
    "            vocab_list[line.split(' ')[0]]=i\n",
    "            i+=1\n",
    "    \n",
    "    return vocab_list\n",
    "\n",
    "human_vocab_file = \"/Users/yihe/Desktop/Stanford/yi_can_summarize/data/cnn/text_vocab.txt\"\n",
    "machine_vocab_file = \"/Users/yihe/Desktop/Stanford/yi_can_summarize/data/cnn/highlight_vocab.txt\"\n",
    "\n",
    "human_vocab, machine_vocab = create_vocab_list(human_vocab_file), create_vocab_list(machine_vocab_file)\n",
    "inv_machine = dict(enumerate(sorted(machine_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vocab in human vocab 10471\n",
      "number of vocab in machine vocab 4423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4421"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"number of vocab in human vocab {}\".format(len(human_vocab.keys()))\n",
    "print \"number of vocab in machine vocab {}\".format(len(machine_vocab.keys()))\n",
    "\n",
    "machine_vocab['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "    \n",
    "    X, Y = zip(*dataset)\n",
    "    \n",
    "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
    "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
    "    \n",
    "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab.keys())), X)))\n",
    "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab.keys())), Y)))\n",
    "    \n",
    "    return X, np.array(Y), Xoh, Yoh\n",
    "\n",
    "def string_to_int(string, length, vocab):\n",
    "    \"\"\"\n",
    "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
    "    input string's characters in the \"vocab\"\n",
    "    \n",
    "    Arguments:\n",
    "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
    "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
    "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
    "    \n",
    "    Returns:\n",
    "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    #make lower to standardize\n",
    "    string = string.split(' ')\n",
    "\n",
    "    if len(string) > length:\n",
    "        string = string[:length]\n",
    "\n",
    "    rep = list(map(lambda x: vocab.get(x,vocab['<UNK>']), string))\n",
    "\n",
    "    if len(string) < length:\n",
    "        rep += [vocab['<PAD>']] * (length - len(string))\n",
    "\n",
    "    #print (rep)\n",
    "    return rep\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Xoh, Yoh = preprocess_data(all_data, human_vocab, machine_vocab, Tx, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X.shape:', (1001, 73))\n",
      "('Y.shape:', (1001, 14))\n",
      "('Xoh.shape:', (1001, 73, 10471))\n",
      "('Yoh.shape:', (1001, 14, 4423))\n"
     ]
    }
   ],
   "source": [
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xoh[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for softmax\n",
    "import keras.backend as K\n",
    "def softmax(x, axis=1):\n",
    "    \"\"\"Softmax activation function.\n",
    "    # Arguments\n",
    "        x : Tensor.\n",
    "        axis: Integer, axis along which the softmax normalization is applied.\n",
    "    # Returns\n",
    "        Tensor, output of softmax transformation.\n",
    "    # Raises\n",
    "        ValueError: In case `dim(x) == 1`.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_step_attention\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a,s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "    e = densor1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = densor2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas =  activator(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([ alphas,a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 's0:0' shape=(?, 64) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input(shape=(n_s,), name='s0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences = True), input_shape = (m, Tx, n_a*2))(X)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c =  post_activation_LSTM_cell(context,initial_state = [s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model([X, s0, c0], outputs = outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 73, 10471)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 73, 64)       2689024     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 73, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 73, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[10][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[11][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[12][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[13][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 73, 10)       1290        concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "                                                                 concatenate_1[10][0]             \n",
      "                                                                 concatenate_1[11][0]             \n",
      "                                                                 concatenate_1[12][0]             \n",
      "                                                                 concatenate_1[13][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 73, 1)        11          dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "                                                                 dense_1[10][0]                   \n",
      "                                                                 dense_1[11][0]                   \n",
      "                                                                 dense_1[12][0]                   \n",
      "                                                                 dense_1[13][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 73, 1)        0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "                                                                 dense_2[10][0]                   \n",
      "                                                                 dense_2[11][0]                   \n",
      "                                                                 dense_2[12][0]                   \n",
      "                                                                 dense_2[13][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[10][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[11][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[12][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[13][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "                                                                 dot_1[10][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[9][2]                     \n",
      "                                                                 dot_1[11][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[10][2]                    \n",
      "                                                                 dot_1[12][0]                     \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[11][2]                    \n",
      "                                                                 dot_1[13][0]                     \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[12][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4423)         287495      lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,010,844\n",
      "Trainable params: 3,010,844\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈2 lines)\n",
    "opt = Adam(lr = 0.005, beta_1 = 0.9, beta_2 = 0.999,decay = 0.01)  \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt,metrics = ['accuracy']) \n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))\n",
    "\n",
    "s0.shape\n",
    "c0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1001/1001 [==============================] - 43s 43ms/step - loss: 113.5936 - dense_3_loss: 7.6568 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 9.9900e-04 - dense_3_acc_2: 9.9900e-04 - dense_3_acc_3: 9.9900e-04 - dense_3_acc_4: 9.9900e-04 - dense_3_acc_5: 0.0030 - dense_3_acc_6: 0.0130 - dense_3_acc_7: 0.0280 - dense_3_acc_8: 0.0649 - dense_3_acc_9: 0.1289 - dense_3_acc_10: 0.2138 - dense_3_acc_11: 0.3377 - dense_3_acc_12: 0.4905 - dense_3_acc_13: 0.6354                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126431c90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 73, 10471)\n",
      "(1001, 64)\n",
      "(1001, 64)\n"
     ]
    }
   ],
   "source": [
    "model.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]]\n",
      "source: it cost more than to develop and doesn t come with French fries on the side\n",
      "output: ['zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg']\n",
      "[[4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]]\n",
      "source: the response to the increasing natural disaster in pakistan is like so much else when it comes to American relations with that country too little and too tentative\n",
      "output: ['zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg']\n",
      "[[4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]\n",
      " [4422]]\n",
      "source: seven Republican presidential contenders faced off Monday in one of the first debates of the primary season, offering policy ideas and criticism of president\n",
      "output: ['zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg', 'zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['it cost more than to develop and doesn t come with French fries on the side',\n",
    "            'the response to the increasing natural disaster in pakistan is like so much else when it comes to American relations with that country too little and too tentative',\n",
    "            'seven Republican presidential contenders faced off Monday in one of the first debates of the primary season, offering policy ideas and criticism of president']\n",
    "\n",
    "for example in EXAMPLES:   \n",
    "    source = [string_to_int(example, Tx, human_vocab)]\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    print prediction\n",
    "    output = [inv_machine[int(i)] for i in prediction]\n",
    "\n",
    "    print \"source: {}\".format(example)\n",
    "    print\"output: {}\".format(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(1, 73, 10471)\n"
     ]
    }
   ],
   "source": [
    "example ='it cost more than to develop and doesn t come with french fries on the side'\n",
    "source = [string_to_int(example, Tx, human_vocab)]\n",
    "print len(source)\n",
    "\n",
    "source_rdy = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "print source_rdy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00079054, 0.00090726, 0.00099326, ..., 0.00013752, 0.00014384,\n",
       "         0.00124767]], dtype=float32),\n",
       " array([[2.0880259e-03, 2.7436044e-03, 3.2198075e-03, ..., 7.8820987e-05,\n",
       "         8.6639528e-05, 5.0747734e-03]], dtype=float32),\n",
       " array([[3.2454950e-03, 4.5919311e-03, 5.6358930e-03, ..., 5.4559827e-05,\n",
       "         6.1446604e-05, 1.0028267e-02]], dtype=float32),\n",
       " array([[3.6280635e-03, 5.2757971e-03, 6.5363408e-03, ..., 4.8720085e-05,\n",
       "         5.5423032e-05, 1.2002281e-02]], dtype=float32),\n",
       " array([[3.7113139e-03, 5.4281582e-03, 6.7351214e-03, ..., 4.7513604e-05,\n",
       "         5.4331780e-05, 1.2443182e-02]], dtype=float32),\n",
       " array([[3.7292594e-03, 5.4611787e-03, 6.7801578e-03, ..., 4.7258414e-05,\n",
       "         5.4105876e-05, 1.2538862e-02]], dtype=float32),\n",
       " array([[3.7335069e-03, 5.4689110e-03, 6.7911176e-03, ..., 4.7198140e-05,\n",
       "         5.4060267e-05, 1.2559942e-02]], dtype=float32),\n",
       " array([[3.7347209e-03, 5.4711783e-03, 6.7943223e-03, ..., 4.7180019e-05,\n",
       "         5.4051245e-05, 1.2565200e-02]], dtype=float32),\n",
       " array([[3.7351786e-03, 5.4720622e-03, 6.7955311e-03, ..., 4.7172616e-05,\n",
       "         5.4049899e-05, 1.2566798e-02]], dtype=float32),\n",
       " array([[3.7354040e-03, 5.4725087e-03, 6.7961281e-03, ..., 4.7168727e-05,\n",
       "         5.4050241e-05, 1.2567441e-02]], dtype=float32),\n",
       " array([[3.7355358e-03, 5.4727769e-03, 6.7964746e-03, ..., 4.7166304e-05,\n",
       "         5.4050732e-05, 1.2567766e-02]], dtype=float32),\n",
       " array([[3.73562193e-03, 5.47294924e-03, 6.79669343e-03, ...,\n",
       "         4.71646999e-05, 5.40511319e-05, 1.25679495e-02]], dtype=float32),\n",
       " array([[3.7356806e-03, 5.4730703e-03, 6.7968466e-03, ..., 4.7163616e-05,\n",
       "         5.4051514e-05, 1.2568068e-02]], dtype=float32),\n",
       " array([[3.7357216e-03, 5.4731509e-03, 6.7969519e-03, ..., 4.7162830e-05,\n",
       "         5.4051750e-05, 1.2568146e-02]], dtype=float32)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([source_rdy, s0, c0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
